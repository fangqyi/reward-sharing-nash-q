critic_hidden_sizes: [64]
learner: "meta_learner"
runner: "dist_episode"
mac: "latent_dist"
total_z_training_steps: 50000
env_steps_every_z: 10000 # 50000
z_sample_runs: 10

epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 50000

buffer_size: 5000

#
meta_t_max:
meta_type: "distance_latent"
latent_relation_space_dim: 4
latent_relation_space_upper_bound: 10.0
latent_relation_space_lower_bound: 0.0
latent_var_dim: 8
latent_encoder_hidden_sizes: [32, 32]
pretrained_task_num: 20
total_pretrain_steps: 1000000
z_critic_lr: 0.0005
z_update_lr: 0.0005

agent: "dgn_agent"


# update the target network every {} episodes
target_update_interval: 200

# use the Q_Learner to train
agent_output_type: "q"

# centralized critic and altruistic agents
centralized_social_welfare: False
separate_agents: False

mutual_infomation_reinforcement: False

is_obs_image: True
# conv params
kernel_size: 3
stride: 1
conv_out_dim: 8
rnn_hidden_dim: 64
name: "pq_dgn_q_learning"
